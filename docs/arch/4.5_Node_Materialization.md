# Node Materialization

## Overview

Node materialization is the final execution step where compiled SQL is run against the data warehouse. For models, this involves invoking materialization macros that handle the specifics of creating tables, views, or incremental updates.

### Example: Table Materialization (Atomic Swap Pattern)

When a model is configured with `materialized='table'`, the materialization macro generates SQL that follows an atomic swap pattern to avoid downtime:

```sql
-- 1. Create a temporary table with the model's SQL
CREATE TABLE "database"."schema"."my_model__dbt_tmp" AS (
  SELECT * FROM ...  -- compiled model SQL
);

-- 2. Drop the existing table (if it exists)
DROP TABLE IF EXISTS "database"."schema"."my_model";

-- 3. Rename the temp table to the final name
ALTER TABLE "database"."schema"."my_model__dbt_tmp" 
RENAME TO "my_model";
```

This pattern ensures that the model is never in a partially-built stateâ€”readers see either the old version or the new version, never an incomplete table. Note that this is a general pattern and assumes DWH support for an atomic `RENAME` operation. Different strategies are implemented for different warehouses based on the capabilities they support with the goals of the materialization in mind.

## ModelRunner.execute()

The main execution method for models (`core/dbt/task/run.py`):

1. Generates runtime context via `generate_runtime_model_context()`
2. Looks up materialization macro via `manifest.find_materialization_macro_by_name()`
3. Invokes the macro via `MacroGenerator`
4. Caches created relations via `adapter.cache_added()`
5. Returns `RunResult`

## Materialization Macro Lookup

When looking up a materialization macro, dbt searches in a specific order (`core/dbt/contracts/graph/manifest.py`):

1. **Adapter-specific, project-local**: `materialization_table_snowflake` in your project
2. **Adapter-specific, imported package**: `materialization_table_snowflake` from a dependency
3. **Adapter-specific, adapter package**: Built into `dbt-snowflake`
4. **Default, project-local**: `materialization_table_default` in your project
5. **Default, core**: `materialization_table_default` from `dbt-core`

The adapter type hierarchy is also considered. For example, Snowflake inherits from the base SQL adapter, so if no Snowflake-specific materialization exists, the default SQL one is used.

## Materialization Macros

Materializations are Jinja macros that define how to create database objects. Built-in materializations live in [`core/dbt/include/global_project/macros/materializations/`](https://github.com/dbt-labs/dbt-adapters/tree/main/dbt-adapters/src/dbt/include/global_project) in the `dbt-adapters` repository.

### Common Materializations

- **view**: `CREATE VIEW AS SELECT ...`
- **table**: `CREATE TABLE AS SELECT ...` (with atomic swap)
- **incremental**: Merge/insert new rows into existing table
- **ephemeral**: No database object (CTE injection only, handled in compilation)

### Materialization Interface

Macros receive a context including:
- `model`: The node being materialized
- `config`: Node configuration (via `config.get()`, `config.require()`)
- `adapter`: Adapter methods for database operations
- `statement()`: Execute SQL and capture results
- `run_query()`: Execute SQL and return results

Must return `{'relations': [relation]}` for cache management.

## The `statement()` Block

The `statement()` block is the core primitive that materializations use to execute SQL. It:

1. Executes the provided SQL against the database via the adapter
2. Captures the result (rows affected, timing, etc.)
3. Stores the result in `sql_results` for later retrieval via `load_result()`

Example usage in a materialization:

```jinja
{% call statement('main') %}
  {{ sql }}
{% endcall %}

{# Later, retrieve the result #}
{% set result = load_result('main') %}
```

The `'main'` statement is special: it's used by `ModelRunner` to extract the adapter response for the `RunResult`.

## Adapter Interactions

Materializations use adapter methods within their jinja definitions for database operations:

- `adapter.execute()`: Run SQL and return response
- `adapter.create_schema()`: Create schema if needed
- `adapter.drop_relation()`: Drop existing object
- `adapter.rename_relation()`: Atomic swap for table replacement
- `adapter.Relation.create()`: Create relation references
- `adapter.get_columns_in_relation()`: Introspect table schema

## Python Models

Python models follow a different execution path than SQL models. Instead of rendering Jinja and executing SQL:

1. The Python code is not Jinja-rendered (no `ref()` in Python syntax)
2. A postfix is appended that calls the `model()` function
3. The materialization macro calls `submit_python_job()` in the context
4. `submit_python_job()` delegates to `adapter.submit_python_job()`
5. The adapter submits the Python code to the warehouse's execution environment (e.g., Snowpark, Databricks notebooks)

Python models are supported only on adapters that implement `submit_python_job()`.

## Seeds and Snapshots

### Seeds

Seeds (`SeedRunner`) load CSV files into database tables:

1. `compile()` is a no-op (returns the node unchanged)
2. `execute()` invokes the seed materialization macro
3. The macro uses `adapter.load_dataframe()` to bulk-insert CSV data
4. Results include the agate table for optional display (`--show`)

### Snapshots

Snapshots (`SnapshotRunner`) implement SCD Type 2 (slowly changing dimensions):

1. Uses the `snapshot` materialization macro
2. Tracks record changes with `dbt_valid_from` and `dbt_valid_to` columns
3. Supports `strategy='timestamp'` (track by updated_at column) or `strategy='check'` (track by column hash)
4. On each run, invalidates changed records and inserts new versions

## Test Execution

Tests are "materialized" by running their SQL and interpreting results:

### Data Tests (Generic and Singular)

1. `TestRunner.execute_data_test()` invokes the `test` materialization
2. The materialization runs the test SQL
3. Results are a single row with columns: `failures`, `should_warn`, `should_error`
4. Row count interpretation: 0 failures = pass, >0 = fail (or warn based on config)

### Unit Tests

1. `TestRunner.execute_unit_test()` builds a special unit test manifest
2. Compiles and runs the `unit` materialization
3. Compares `actual` vs `expected` result sets
4. Reports diff if mismatch occurs

## Incremental Strategies

Incremental models support multiple strategies:

- **append**: Insert all new rows
- **delete+insert**: Delete matching rows, then insert new
- **merge**: Upsert based on unique key (requires adapter support)
- **microbatch**: Process in time-based batches (see `MicrobatchModelRunner`)

Strategy selection based on `config.incremental_strategy` and adapter capabilities.

## Hook Execution

RunTask executes project hooks around model execution:

- `on-run-start`: Before any models run
- `on-run-end`: After all models complete
- Pre/post hooks on individual models via `config.pre_hook` / `config.post_hook`

## Relation Cache Management

dbt maintains an in-memory cache of database relations to avoid repeated introspection queries. Materializations must keep this cache in sync:

- `adapter.cache_added(relation)`: Register a newly created relation
- `adapter.cache_dropped(relation)`: Remove a dropped relation
- `adapter.cache_renamed(from_relation, to_relation)`: Update after rename

The `_materialization_relations()` return value (`{'relations': [...]}`) is used by `ModelRunner` to automatically call `cache_added()` for each relation.

## Result Handling

`RunResult` captures execution outcome:
- `status`: Success, Error, Skipped, etc.
- `timing`: Compile and execute timing info
- `adapter_response`: Database-specific response (rows affected, etc.)
- `message`: Human-readable status message
