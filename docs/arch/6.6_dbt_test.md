# dbt test

## Overview

`dbt test` runs data tests and unit tests defined in your project. Data tests validate assumptions about your data (e.g., uniqueness, not null), while unit tests validate model logic with fixed inputs.

**Reference**: https://docs.getdbt.com/reference/commands/test

## Task Class: `TestTask`

`TestTask` extends `RunTask` and handles both singular tests, generic tests, and unit tests.

```python
class TestTask(RunTask):    
    @property
    def resource_types(self) -> List[NodeType]:
        # TEST_NODE_TYPES = [NodeType.Test, NodeType.Unit]
        return [rt for rt in resource_types if rt in TEST_NODE_TYPES]
```

### TestRunner

`TestRunner` extends `CompileRunner` and handles three test types:

1. **Singular Tests**: Custom SQL test files in `tests/` directory
2. **Generic Tests**: Schema tests like `unique`, `not_null` from YAML
3. **Unit Tests**: Tests with mocked inputs defined in YAML

## Implementation Quirks

### Test Execution Path Split

`TestRunner.execute()` branches based on test type:

```python
def execute(self, test, manifest):
    if isinstance(test, UnitTestDefinition):
        unit_test_node, result = self.execute_unit_test(test, manifest)
        return self.build_unit_test_run_result(unit_test_node, result)
    else:
        test_result = self.execute_data_test(test, manifest)
        return self.build_test_run_result(test, test_result)
```

### Unit Test Manifest Loading

Unit tests require a separate mini-manifest containing mocked versions of referenced models:

```python
def execute_unit_test(self, unit_test_def, manifest):
    # Build isolated manifest with just this unit test
    loader = UnitTestManifestLoader(manifest, self.config, {unit_test_def.unique_id})
    unit_test_manifest = loader.load()
    
    # Compile and execute with the mocked manifest
    unit_test_node = self.compiler.compile_node(unit_test_node, unit_test_manifest, {})
    ...
```

### Unit Test Diff Rendering

Unit tests compare actual vs expected results using the `daff` library for diff visualization:

```python
def _get_daff_diff(self, expected, actual):
    expected_table = daff.PythonTableView(list_rows_from_table(expected, sort=True))
    actual_table = daff.PythonTableView(list_rows_from_table(actual, sort=True))
    diff = daff.TableDiff(alignment, flags)
    return diff
```

The diff is rendered with colors (`green` for actual, `red` for expected).

### Test Result Status Logic

Data test status depends on the `severity` config (`error` or `warn`) and the failure count thresholds (`error_if`, `warn_if`):

- If severity is `ERROR` and failures exceed `error_if` → **Fail**
- If failures exceed `warn_if` → **Warn** (or **Fail** if `--warn-error` is set)
- Otherwise → **Pass**

### `--store-failures`

When enabled, test failures are stored in the database for later analysis. The `test` materialization creates a table with failing rows.

## Flags

| Flag | Description |
|------|-------------|
| `--select` | Select tests to run |
| `--exclude` | Exclude tests |
| `--store-failures` | Persist test failures to the database |
| `--resource-type` | Filter by `test` or `unit` |

## Test Configuration

```yaml
models:
  - name: orders
    columns:
      - name: order_id
        tests:
          - unique
          - not_null
      - name: status
        tests:
          - accepted_values:
              values: ['placed', 'shipped', 'delivered']

unit_tests:
  - name: test_order_total
    model: orders
    given:
      - input: ref('order_items')
        rows:
          - {order_id: 1, amount: 10}
          - {order_id: 1, amount: 20}
    expect:
      rows:
        - {order_id: 1, total: 30}
```
